{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d191c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Komoran\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b06743",
   "metadata": {},
   "source": [
    "### 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29331ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 댓글을 수집하기 위해 댓글이 모두 나올때까지 스크롤을 내려주는 함수\n",
    "def scroll_down_comment(driver):\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    # 화면이 스크롤을 내려도 똑같을 때까지 계속해서 내림\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# 영상 링크 수집을 위한 스크롤 내리는 함수\n",
    "def scroll_down(driver):\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    # 유튜브 영상은 계속해서 나오므로 10번만 스크롤을 내려 나오는 영상 링크만 수집하도록 함\n",
    "    for _ in range(10):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        \n",
    "        # 만약 그 전에 스크롤이 멈추게 된다면 반복을 중지\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "# 유튜브 영상에 존재하는 '더보기' 버튼을 클릭하여 조회수와 업로드 날짜 등을 확인하기 위한 함수\n",
    "def click_more_button(driver):\n",
    "    try:\n",
    "        more_button = driver.find_element(By.XPATH, '//tp-yt-paper-button[@class=\"button style-scope ytd-text-inline-expander\" and @id=\"expand\"]')\n",
    "        if more_button:\n",
    "            more_button.click()\n",
    "            time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"더보기 버튼을 찾을 수 없습니다.\")\n",
    "        \n",
    "# int로 변환이 가능한지 아닌지를 판단하여 True or False를 return하는 함수\n",
    "def is_int_convertible(value):\n",
    "    try:\n",
    "        int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# 영상의 채널명, 구독자 수, 조회수, 좋아요 수, 댓글 개수를 추출하는 함수        \n",
    "def get_video_details(soup):\n",
    "    channel_name_element = soup.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\")\n",
    "    channel_name = [i.text for i in channel_name_element][0] if channel_name_element else \"Unknown Channel Name\"\n",
    "\n",
    "    subscriber_count_element = soup.find(\"yt-formatted-string\", class_=\"style-scope ytd-video-owner-renderer\")\n",
    "    subscriber_count = subscriber_count_element.text.strip() if subscriber_count_element else \"Unknown Subscriber Count\"\n",
    "\n",
    "    view_count_element = soup.find_all(\"span\", class_=\"style-scope yt-formatted-string bold\")\n",
    "    view_count = [element.text.strip() for element in view_count_element][0] if view_count_element else \"Unknown View Count\"\n",
    "    \n",
    "    like_count_elements = soup.find_all(\"span\", class_=\"yt-core-attributed-string yt-core-attributed-string--white-space-no-wrap\")\n",
    "    like_count = [i.text for i in like_count_elements if i.text[-1] in ['천','만'] or is_int_convertible(i.text)][0]\n",
    "\n",
    "    comment_count_element = soup.find(\"yt-formatted-string\", class_=\"count-text style-scope ytd-comments-header-renderer\")\n",
    "    comment_count = comment_count_element.text.strip() if comment_count_element else \"Unknown Comment Count\"\n",
    "\n",
    "    return channel_name, subscriber_count, view_count, like_count, comment_count\n",
    "        \n",
    "# 영상의 제목, 업로드 날짜, 채널명, 구독자 수, 조회수, 좋아요 수, 댓글 개수, 댓글을 추출하는 함수 \n",
    "def get_comments(driver, video_link):\n",
    "    driver.get(video_link) # 영상 링크에 접속\n",
    "    time.sleep(5) # 화면이 나올 때까지 대기\n",
    "    \n",
    "    click_more_button(driver) # 더보기 버튼 클릭\n",
    "    \n",
    "    scroll_down(driver) # 모든 댓글이 나오도록 스크롤을 끝까지 내리기\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    title_element = soup.find(\"h1\", class_=\"title style-scope ytd-video-primary-info-renderer\")\n",
    "    title = title_element.text.strip() if title_element else \"Unknown Title\"\n",
    "    comments = soup.find_all(\"yt-formatted-string\", class_=\"style-scope ytd-comment-renderer\")\n",
    "    \n",
    "    upload_date_elements = soup.find_all(\"span\", class_=\"style-scope yt-formatted-string bold\")\n",
    "    upload_date = [element.text.strip() for element in upload_date_elements][2] if upload_date_elements else \"Unknown Upload Date\"\n",
    "    \n",
    "    channel_name, subscriber_count, view_count, like_count, comment_count = get_video_details(soup) # 앞서 정의한 함수를 통해 정보 추출\n",
    "\n",
    "    return title, upload_date, channel_name, subscriber_count, view_count, like_count, comment_count, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad58363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드라이버 path 설정\n",
    "driver_path = \"C:\\\\Users\\\\rltjq\\\\chromedriver.exe\"\n",
    "\n",
    "# 검색어 지정\n",
    "search_query = \"갤럭시 리뷰\"\n",
    "\n",
    "# 드라이버 생성\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "# 유튜브 링크 접속\n",
    "driver.get(\"https://www.youtube.com\")\n",
    "\n",
    "# 화면이 나올 때까지 대기\n",
    "time.sleep(5)\n",
    "\n",
    "# 검색창에 지정한 검색어 입력하고 검색\n",
    "search_box = driver.find_element(By.XPATH, \"//input[@id='search']\")\n",
    "search_box.send_keys(search_query)\n",
    "search_box.submit()\n",
    "\n",
    "# 화면이 나올 때까지 대기\n",
    "time.sleep(5)\n",
    "\n",
    "# 스크롤을 내려 수집할 영상 생성\n",
    "scroll_down(driver)\n",
    "\n",
    "# beautifulsoup으로 html을 불러옴\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# html 파일 내에 존재하는 모든 영상 링크를 수집\n",
    "video_links = set()\n",
    "for link in soup.find_all(\"a\", id=\"thumbnail\"):\n",
    "    href = link.get(\"href\")\n",
    "    \n",
    "    # short 영상은 다른 영상과 포맷이 달라 short 영상은 제거\n",
    "    if href is not None and 'shorts' not in href:\n",
    "        video_links.add(\"https://www.youtube.com\" + href)\n",
    "\n",
    "# 수집한 정보를 저장하기 위한 csv 파일 생성 후, 영상마다 정보 수집\n",
    "with open(\"galaxy_review_comment.csv\", \"w\", encoding=\"utf-8\", newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Video Title\", \"Upload_date\", \"Channel_name\", \"Subscriber_count\", \"View_count\", 'Like_count', 'Comment_count',\n",
    "                         \"Comment\"])\n",
    "\n",
    "    for video_link in video_links:\n",
    "        title, upload_date, channel_name, subscriber_count, view_count, like_count, comment_count, comments = get_comments(driver, video_link)\n",
    "        for comment in comments:\n",
    "            csv_writer.writerow([title, upload_date, channel_name, subscriber_count, view_count, like_count, comment_count, comment.text.strip()])\n",
    "        print(f\"Comments from '{title}' have been saved\")\n",
    "\n",
    "print(\"All comments have been saved to 'comments.csv'\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42018007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 갤럭시 리뷰와 모두 동일, 검색어만 변경\n",
    "\n",
    "driver_path = \"C:\\\\Users\\\\rltjq\\\\chromedriver.exe\"\n",
    "\n",
    "search_query = \"애플 리뷰\"\n",
    "\n",
    "driver = webdriver.Chrome(driver_path)\n",
    "\n",
    "driver.get(\"https://www.youtube.com\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "search_box = driver.find_element(By.XPATH, \"//input[@id='search']\")\n",
    "search_box.send_keys(search_query)\n",
    "search_box.submit()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "scroll_down(driver)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "video_links = set()\n",
    "for link in soup.find_all(\"a\", id=\"thumbnail\"):\n",
    "    href = link.get(\"href\")\n",
    "    if href is not None and 'shorts' not in href:\n",
    "        video_links.add(\"https://www.youtube.com\" + href)\n",
    "\n",
    "with open(\"apple_review_comment.csv\", \"w\", encoding=\"utf-8\", newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Video Title\", \"Upload_date\", \"Channel_name\", \"Subscriber_count\", \"View_count\", 'Like_count', 'Comment_count',\n",
    "                         \"Comment\"])\n",
    "\n",
    "    for video_link in video_links:\n",
    "        title, upload_date, channel_name, subscriber_count, view_count, like_count, comment_count, comments = get_comments(driver, video_link)\n",
    "        for comment in comments:\n",
    "            csv_writer.writerow([title, upload_date, channel_name, subscriber_count, view_count, like_count, comment_count, comment.text.strip()])\n",
    "        print(f\"Comments from '{title}' have been saved\")\n",
    "\n",
    "print(\"All comments have been saved to 'comments.csv'\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bca611",
   "metadata": {},
   "source": [
    "### 데이터 전처리(갤럭시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92615f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "galaxy = pd.read_csv('galaxy_review_comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어로만 이루어진 댓글 삭제 + 결측치 삭제\n",
    "pattern = r'^[a-zA-Z\\s\\W\\d]*$'\n",
    "\n",
    "galaxy['Comment'] = galaxy['Comment'].apply(lambda x: x if type(x) != str or not re.match(pattern, x) else pd.NA)\n",
    "\n",
    "galaxy = galaxy.dropna(subset=['Comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ded63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가디언즈 오브 갤럭시 영화와 관련된 리뷰 영상 삭제\n",
    "keywords = ['가디언즈', '가오갤', '마블', '그루트']\n",
    "\n",
    "for keyword in keywords:\n",
    "    galaxy = galaxy[~galaxy['Video Title'].str.contains(keyword, na=False)]\n",
    "    \n",
    "galaxy = pd.DataFrame(galaxy.values, columns=galaxy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65752a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상의 시간을 표시한 부분은 모두 삭제\n",
    "def remove_time(text):\n",
    "    return re.sub(r'\\b(\\d{1,2}):(\\d{2})\\b', '', text)\n",
    "\n",
    "galaxy['Comment'] = galaxy['Comment'].apply(remove_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ef5f4",
   "metadata": {},
   "source": [
    "### 데이터 전처리(애플)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv('apple_review_comment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어로만 이루어진 댓글 삭제 + 결측치 삭제\n",
    "pattern = r'^[a-zA-Z\\s\\W\\d]*$'\n",
    "\n",
    "apple['Comment'] = apple['Comment'].apply(lambda x: x if type(x) != str or not re.match(pattern, x) else pd.NA)\n",
    "\n",
    "apple = apple.dropna(subset=['Comment'])\n",
    "\n",
    "apple = pd.DataFrame(apple.values, columns=apple.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157766ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 애플 TV와 연관되어 있는 영화나 드라마에 대한 리뷰 삭제\n",
    "def video_title_filter(text):\n",
    "    if '애플' not in text or 'TV' in text:\n",
    "        if '아이폰' in text:\n",
    "            return None\n",
    "        elif '맥' in text:\n",
    "            return None\n",
    "        elif '워치' in text:\n",
    "            return None\n",
    "        \n",
    "        filter_list.append(text)\n",
    "        return None\n",
    "    \n",
    "filter_list = []\n",
    "apple['Video Title'].apply(video_title_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = []\n",
    "for i in range(len(apple)):\n",
    "    if apple['Video Title'][i] in filter_list:\n",
    "        drop_idx.append(i)\n",
    "\n",
    "apple.drop(drop_idx, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상의 시간을 표시한 부분은 모두 삭제\n",
    "apple['Comment'] = apple['Comment'].apply(remove_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fefb09",
   "metadata": {},
   "source": [
    "### 맞춤법 검사 및 띄어쓰기 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8089bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_replace(text, error, candWord):\n",
    "    return text.replace(error, candWord)\n",
    "\n",
    "def spell_correct(text):\n",
    "    # 1. 텍스트 준비 & 개행문자 처리\n",
    "    text = text.replace('\\n', '\\r\\n')\n",
    "    # 2. 맞춤법 검사 요청 (requests)\n",
    "    response = requests.post('http://164.125.7.61/speller/results', data={'text1': text})\n",
    "    # 3. 응답에서 필요한 내용 추출 (html 파싱)\n",
    "    data = response.text.split('data = [', 1)[-1].rsplit('];', 1)[0]\n",
    "    \n",
    "    try: # 맞춤법이 틀린 것이 존재하지 않을 수 있어 try-except 문으로 해결\n",
    "        data = json.loads(data)\n",
    "    \n",
    "        for err in data['errInfo']:\n",
    "            text = text_replace(text, err['orgStr'], err['candWord'].split('|')[0])\n",
    "        \n",
    "        text = text.replace('\\r\\n', '\\n')\n",
    "        \n",
    "    except:\n",
    "        text = text.replace('\\r\\n', '\\n')\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ccc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "galaxy['Comment'] = galaxy['Comment'].progress_apply(spell_correct)\n",
    "apple['Comment'] = apple['Comment'].progress_apply(spell_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4788bc",
   "metadata": {},
   "source": [
    "### 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9434bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "komoran = Komoran()\n",
    "\n",
    "def tokenizer(text):\n",
    "    try: # 이모티콘과 같이 토큰화가 불가능한 문장은 건너뛰고 수행\n",
    "        return komoran.pos(text)\n",
    "    except:\n",
    "        return pd.NA\n",
    "    \n",
    "galaxy['token'] = galaxy['Comment'].progress_apply(tokenizer)\n",
    "apple['token'] = apple['Comment'].progress_apply(tokenizer)\n",
    "\n",
    "galaxy = galaxy.dropna(subset=['token'])\n",
    "apple = apple.dropna(subset=['token'])\n",
    "\n",
    "galaxy = pd.DataFrame(galaxy.values, columns=galaxy.columns)\n",
    "apple = pd.DataFrame(apple.values, columns=apple.columns)\n",
    "\n",
    "galaxy.to_csv('galaxy_review_token.csv', index=False)\n",
    "apple.to_csv('apple_review_token.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa70cf",
   "metadata": {},
   "source": [
    "### 워드클라우드 생성(갤럭시)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b087b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "galaxy = pd.read_csv('galaxy_review_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "tqdm.pandas()\n",
    "galaxy_word_dict = {} # 갤럭시 단어 사전 정의\n",
    "\n",
    "stopwords = pd.read_csv('koreanStopwords.txt')\n",
    "stopwords = list(stopwords['0']) # stopwords 불러와서 저장\n",
    "\n",
    "del_tag = ['JKS','JKC','JKG','JKO','JKB','JKV',\n",
    "'JKQ','JX','JC','EP','EF','EC','ETN','ETM',\n",
    "'XPN','XSN','XSV','XSA','XR','SF','SP','SS',\n",
    "'SE','SO','SH','SW','NF','NV','SN','NA'] # 사용하지 않을 품사 지정\n",
    "\n",
    "\n",
    "def create_dict(lst):\n",
    "    str_list = lst\n",
    "    list_obj = ast.literal_eval(str_list) # 기존에 str 형태로 저장되어 있던 것을 list 형태로 변환\n",
    "    \n",
    "    for i in list_obj:\n",
    "        if i[1] not in del_tag and i[0] not in stopwords: # 기존 사전에 존재하는 단어는 +1을, 없던 단어는 1로 생성\n",
    "            try:\n",
    "                galaxy_word_dict[i[0]] += 1\n",
    "            except:\n",
    "                galaxy_word_dict[i[0]] = 1\n",
    "\n",
    "galaxy['token'].progress_apply(create_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7962e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "font_path = 'C:\\\\Users\\\\rltjq\\\\anaconda3\\\\envs\\\\text\\\\Lib\\\\site-packages\\\\matplotlib\\\\mpl-data\\\\fonts\\\\ttf\\\\NanumGothic.ttf'\n",
    "\n",
    "freq = galaxy_word_dict\n",
    "cloud = WordCloud(font_path = font_path,\n",
    "                  background_color='white',\n",
    "                  width=800, height=800)\n",
    "\n",
    "galaxy_cloud = cloud.generate_from_frequencies(freq)\n",
    "\n",
    "arr = galaxy_cloud.to_array()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(arr)\n",
    "plt.savefig('galaxy_wordcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67476c4",
   "metadata": {},
   "source": [
    "### 워드클라우드 생성(애플)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e01323",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = pd.read_csv('apple_review_token.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebfb271",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "apple_word_dict = {}\n",
    "\n",
    "stopwords = pd.read_csv('koreanStopwords.txt')\n",
    "stopwords = list(stopwords['0'])\n",
    "\n",
    "del_tag = ['JKS','JKC','JKG','JKO','JKB','JKV',\n",
    "'JKQ','JX','JC','EP','EF','EC','ETN','ETM',\n",
    "'XPN','XSN','XSV','XSA','XR','SF','SP','SS',\n",
    "'SE','SO','SH','SW','NF','NV','SN','NA']\n",
    "\n",
    "\n",
    "def create_dict(lst):\n",
    "    str_list = lst\n",
    "    list_obj = ast.literal_eval(str_list)\n",
    "    \n",
    "    for i in list_obj:\n",
    "        if i[1] not in del_tag and i[0] not in stopwords:\n",
    "            try:\n",
    "                apple_word_dict[i[0]] += 1\n",
    "            except:\n",
    "                apple_word_dict[i[0]] = 1\n",
    "                \n",
    "apple['token'].progress_apply(create_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = apple_word_dict\n",
    "cloud = WordCloud(font_path = font_path,\n",
    "                  background_color='white',\n",
    "                  width=800, height=800)\n",
    "\n",
    "apple_cloud = cloud.generate_from_frequencies(freq)\n",
    "\n",
    "arr = apple_cloud.to_array()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.imshow(arr)\n",
    "plt.savefig('apple_wordcloud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6a5d4",
   "metadata": {},
   "source": [
    "### text classification 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f3264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "galaxy['label'] = 0\n",
    "apple['label'] = 1\n",
    "all_review = pd.concat([galaxy, apple])\n",
    "\n",
    "all_review = all_review.loc[:, ['token','label']]\n",
    "all_review = all_review.dropna()\n",
    "train, test = train_test_split(all_review, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4845f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "import torch\n",
    "\n",
    "# 입력 받은 텍스트를 단어 별로 분리하고, 첫 번째 요소(vocab)만 반환\n",
    "def tokenize(text):\n",
    "    return [i[0] for i in text]\n",
    "\n",
    "# Field 객체를 정의\n",
    "# TEXT : sequential=True로 설정하여 시퀀스 데이터임을 명시\n",
    "#        tokenize 함수는 위에서 정의한 tokenize 함수로 설정\n",
    "#        batch_first=True로 설정하여 배치 차원을 맨 앞으로 가져옴\n",
    "#        use_vocab=True로 설정하여 어휘 사전을 사용\n",
    "# LABEL : sequential=False로 설정하여 시퀀스 데이터가 아님을 명시\n",
    "#         use_vocab=False로 설정하여 어휘 사전을 사용하지 않음\n",
    "#         preprocessing 인자를 float로 설정하여 레이블 데이터를 실수로 변환\n",
    "#         dtype을 torch.float로 설정하여 텐서의 데이터 타입을 실수로 설정\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenize, batch_first=True, use_vocab=True)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False, preprocessing=float, dtype=torch.float)\n",
    "\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "\n",
    "# 저장된 CSV 파일을 이용하여 TabularDataset 객체 생성\n",
    "# train.csv와 test.csv에 대해 각각 TabularDataset 객체 생성\n",
    "fields = [('text', TEXT), ('label', LABEL)]\n",
    "train_data, = data.TabularDataset.splits(\n",
    "        path = '', # 현재 디렉토리\n",
    "        train = 'train.csv', # 훈련 데이터 파일 이름\n",
    "        format = 'csv', # 파일 형식\n",
    "        fields = fields, # 사용할 필드\n",
    "        skip_header=True # 헤더 건너뛰기\n",
    ")\n",
    "test_data, = data.TabularDataset.splits(\n",
    "        path = '', # 현재 디렉토리\n",
    "        train = 'test.csv', # 테스트 데이터 파일 이름\n",
    "        format = 'csv', # 파일 형식\n",
    "        fields = fields, # 사용할 필드\n",
    "        skip_header=True # 헤더 건너뛰기\n",
    ")\n",
    "\n",
    "# 어휘 사전을 생성\n",
    "# 훈련 데이터에 있는 단어를 최대 25000개까지만 사용하도록 제한\n",
    "TEXT.build_vocab(train_data, max_size=25000)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eeaedb",
   "metadata": {},
   "source": [
    "## text classifiction 모델 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cce22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "from torch import nn, optim\n",
    "\n",
    "# Iterator 생성\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data), \n",
    "    batch_size = BATCH_SIZE, \n",
    "    sort_within_batch = True, # 배치 내에서 데이터를 정렬\n",
    "    sort_key = lambda x: len(x.text), # 정렬 기준을 지정\n",
    "    device = device)\n",
    "\n",
    "# 모델 정의\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1, :, :]\n",
    "        output = self.fc(self.dropout(hidden))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187bc27",
   "metadata": {},
   "source": [
    "## Optimizer 및 손실함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 설정\n",
    "vocab_size = len(TEXT.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                       bidirectional, dropout).to(device)\n",
    "\n",
    "# 옵티마이저와 손실 함수 설정\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    # 반올림한 예측값과 실제 값이 같은지 확인\n",
    "    correct = (torch.round(torch.sigmoid(preds)) == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077b8be",
   "metadata": {},
   "source": [
    "## 학습 & 평가 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a864f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 테스트 함수 정의\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dab559",
   "metadata": {},
   "source": [
    "## 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6fb25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습 및 테스트 진행\n",
    "n_epochs = 50\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    train(model, train_iterator, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, test_iterator, criterion)\n",
    "    \n",
    "    loss_list.append(val_loss)\n",
    "    acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1d47bb",
   "metadata": {},
   "source": [
    "## loss & acc 결과 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476cbdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(1,51, 50)\n",
    "plt.plot(x, loss_list)\n",
    "plt.title('LSTM Loss Graph')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('loss_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab502ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(1,51, 50)\n",
    "plt.plot(x, acc_list)\n",
    "plt.title(\"LSTM Accuracy Graph\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.savefig('acc_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a0525",
   "metadata": {},
   "source": [
    "## 모델 저장 및 Inference 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 저장\n",
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df744ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 불러오기\n",
    "model.load_state_dict(torch.load('model.pt'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Code\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    komoran = Komoran()\n",
    "    tokenized = komoran.morphs(sentence)  # 문장을 토큰화\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]  # 각 토큰을 정수로 변환\n",
    "    length = [len(indexed)]  # 한 문장의 길이\n",
    "    tensor = torch.LongTensor(indexed).to(device)  # 변환된 정수를 텐서로 변환\n",
    "    tensor = tensor.unsqueeze(1).T  # 배치 차원을 추가\n",
    "    length_tensor = torch.LongTensor(length)  # 문장 길이를 텐서로 변환\n",
    "    prediction = torch.sigmoid(model(tensor))  # 예측\n",
    "    return prediction.item()  # 예측 결과를 반환\n",
    "\n",
    "print(predict_sentiment(model, \"아이폰 14 사고싶다\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
